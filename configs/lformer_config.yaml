optimizer:
  name: "adamw"
  lr: !!float 3e-4
  weight_decay: !!float 1e-6

lr_scheduler:
  name: 'cos_anneal_restart_reduce'
  T_0: 100
  T_mult: 2
  lr_mult: 0.5
  eta_min: !!float 1e-6

max_norm: 0.03
ema_decay: 0.999

path:
  wv3_train_path: "/Data2/ZiHanCao/datasets/pansharpening/wv3/training_wv3/train_wv3.h5"
  wv3_val_path: "/Data2/ZiHanCao/datasets/pansharpening/wv3/reduced_examples/test_wv3_multiExm1.h5"

  gf2_train_path: "/Data2/ZiHanCao/datasets/pansharpening/gf/training_gf2/train_gf2.h5"
  gf2_val_path: "/Data2/ZiHanCao/datasets/pansharpening/gf/reduced_examples/test_gf2_multiExm1.h5"

  cave_x4_train_path: "/Data2/ZiHanCao/datasets/HISI/new_cave/train_cave(with_up)x4.h5"
  cave_x4_val_path: "/Data2/ZiHanCao/datasets/HISI/new_cave/test_cave(with_up)x4.h5"

network_configs:
  lformer:
    pan_dim: 1
    lms_dim: 4
    attn_dim: 64
    hp_dim: 64
    n_stage: 5
    patch_merge: yes
    crop_batch_size: 64
    patch_size_list: [16, 64, 64]
    scale: 4

logger_config:
  base_path: ./log_file/
  name: lformer
  file_mode: w
